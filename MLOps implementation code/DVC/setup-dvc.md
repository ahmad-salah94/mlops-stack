# DVC Setup

This project utilizes DVC (Data Version Control) for versioning and tracking datasets in the machine learning pipeline. The following steps were taken to set up DVC:

## Installation
DVC was installed using the command `pip install dvc`.

## Initialization
Inside the project directory, the command `dvc init` was executed to initialize DVC and convert the directory into a DVC repository.

## Git Ignore
The `.dvc` directories and files generated by DVC were added to the `.gitignore` file to prevent temporary or local configuration files from being checked into the Git repository.

## Remote Storage
An Amazon S3 bucket was added as a remote storage using the command `dvc remote add -d myremote s3://my-bucket/path`. The access credentials for Amazon S3 were securely stored in the DVC configuration file to enable access to the bucket.

## Data Versioning
Important datasets were added to DVC tracking using the `dvc add` command. This created `.dvc` files containing versioning information for each dataset. Changes were committed to Git, including the `.dvc` files representing the metadata of the tracked datasets. Data was pushed to the S3 bucket using `dvc push` for secure and versioned storage. Team members can download specific versions of datasets using the `dvc pull` command.

## Data Updates
When a new version of a dataset is developed, data scientists can version and store it in the S3 bucket using the `dvc add` and `dvc push` commands.

The combination of DVC and Amazon S3 enhances experiment reproducibility and improves transparency within the team. DVC facilitates the retrieval of previous versions of data and models, while Amazon S3 optimizes data storage security, cost-efficiency, and scalability. This integration has strengthened data management and laid the foundation for optimized data management and versioning in the 'Oper-AI-tion' project.